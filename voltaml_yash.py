# -*- coding: utf-8 -*-
"""VoltaML_Yash.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EyZNeH6NTIbkdOvXdzRpRrW67t3qfYc4
"""

#Importing Libraries
import os
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
import cv2
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization,Flatten
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras import regularizers
from tensorflow.keras.models import Model

os.listdir(r'D:\Kaggel\archive\natural_images')

def img_read(folder):
    imgg=[] 
    labels = []
    classes = ['person', 'airplane', 'motorbike', 'fruit', 'flower', 'car', 'cat', 'dog']
    for i in classes:
        for filename in tqdm(os.listdir(os.path.join(folder , i))):
            img = cv2.imread(os.path.join(os.path.join(folder,i) , filename)) #Reading the image file from the base directory
            img = cv2.resize(img , (150 , 150)) # Resizing the images to a matrix of (150,150)
            imgg.append(img) #Storing the Image one by one in an empty list
            labels.append(i) #Storing the labels of the image one by one in an empty list 
    return imgg , labels

img , label = img_read(r'D:\Kaggel\archive\natural_images')


#Creating data generator training and validation dataset
dir = r'D:\Kaggel\archive\natural_images'
data_gen = ImageDataGenerator(rescale = 1./255,
                              horizontal_flip=True,
                              brightness_range=[0.4,1.5],# values less then 1 darken the image and greater than 1 brighten.
                              validation_split=0.2)
train_datagen = data_gen.flow_from_directory(dir,
                                         batch_size = 64,
                                         class_mode = "categorical",
                                         target_size = (150,150),
                                         subset = "training")
val_datagen = data_gen.flow_from_directory(
    dir,
    batch_size =64 ,
    class_mode = "categorical",
    target_size = (150,150),
    subset = "validation"
)

#Model Construction 
inputs = tf.keras.Input(shape=(150,150,3))
x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
x = Conv2D(32, (3, 3), activation='relu', padding='same',)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Dropout(0.2)(x)
x = Conv2D(32, (3, 3), activation='relu', padding='same',)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
x = Dropout(0.2)(x)
x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
x = Flatten()(x)
x = Dense(16,activation='relu')(x)
output = Dense(8, activation='softmax')(x)
model = Model(inputs, output)

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#Callbacks
EP = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5
)
MCP = tf.keras.callbacks.ModelCheckpoint(
    filepath = './weights' ,
    monitor='accuracy' , 
    save_best_only=True
)

model.summary()

history = model.fit(
    train_datagen,
    batch_size=128,
    epochs=5,
    #steps_per_epoch=100,
    verbose=1,
    validation_data=val_datagen,
    callbacks=[EP,MCP]
)
model.save("model.h5")
print('model saved')
